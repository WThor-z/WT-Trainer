from contextlib import contextmanager
import gc

import torch


def create_tensors():
    return {
        "test1": torch.randn((151936, 2560), dtype=torch.bfloat16, requires_grad=False),
        "test2": torch.randn((2560, 4096), dtype=torch.bfloat16, requires_grad=False),
        "test3": torch.randn((2560, 1024), dtype=torch.bfloat16, requires_grad=False),
        "test4": torch.randn((2560, 9728), dtype=torch.bfloat16, requires_grad=False),
        "test5": torch.randn((1, 2560), dtype=torch.bfloat16, requires_grad=False),
    }


def get_cuda_tensors():
    tensors = []
    for obj in gc.get_objects():
        try:
            if torch.is_tensor(obj) and obj.is_cuda:
                tensors.append(obj)
        except Exception:
            pass
    return tensors


@contextmanager
def search_tensors():
    before = {id(t): t for t in get_cuda_tensors()}
    before_mem = torch.cuda.memory_allocated()

    yield

    after_mem = torch.cuda.memory_allocated()
    after = {id(t): t for t in get_cuda_tensors()}
    new_tensors = {k: v for k, v in after.items() if k not in before}
    print(f"\nğŸ” å‘ç° {len(new_tensors)} ä¸ªæ–°å¢ CUDA å¼ é‡ï¼š")
    total_bytes = 0
    for i, (tid, t) in enumerate(new_tensors.items()):
        numel = t.numel()
        element_size = t.element_size()  # æ¯ä¸ªå…ƒç´ å å¤šå°‘å­—èŠ‚ï¼ˆå¦‚ float32 â†’ 4ï¼‰
        size_bytes = numel * element_size
        total_bytes += size_bytes
        print(
            f"  [{i+1}] id={tid} | shape={list(t.shape)} | dtype={t.dtype} | "
            f"numel={numel:>8} | size={size_bytes:>8} B ({size_bytes/1024:.1f} KB)| total_memory={after_mem-before_mem} B"
        )


def test_my_method(test_tensors):
    """é€ä¸ªåˆ†é…åˆ° GPU"""
    for key in test_tensors:
        print(f"---æ­£åœ¨åŠ è½½{key}---")
        test_tensors[key] = test_tensors[key].to("cuda")
    return test_tensors


def test_hf_method(test_tensors):
    """å…ˆé¢„çƒ­å†åˆ†é…"""
    big = torch.empty([160000, 2700], dtype=torch.bfloat16, device="cuda")
    del big
    for key in test_tensors:
        print(f"---æ­£åœ¨åŠ è½½{key}---")
        test_tensors[key] = test_tensors[key].to("cuda")
    return test_tensors


def test_torch_allocated(test_tensors):
    """æµ‹è¯•Pytorchå¦‚ä½•åˆ†é…å†…å­˜"""
    for key in test_tensors:
        print(f"---æ­£åœ¨åŠ è½½{key}---")
        before_reserved = torch.cuda.memory_reserved()
        before_allocated = torch.cuda.memory_allocated()
        test_tensors[key] = test_tensors[key].to("cuda")
        after_reserved = torch.cuda.memory_reserved()
        after_allocated = torch.cuda.memory_allocated()
        print(
            f"å¼ é‡ç†è®ºå†…å­˜ : {test_tensors[key].nbytes}, å®é™…å ç”¨å†…å­˜ : {after_allocated-before_allocated}, å®é™…åˆ†é…å†…å­˜ : {after_reserved-before_reserved}"
        )
    return test_tensors


def test_warmup(test_tensors):
    """
    é€šè¿‡ GPU è™šæ‹Ÿåœ°å€æµ‹è¯•å¼ é‡æ˜¯å¦è½åœ¨é¢„çƒ­å¤§å†…å­˜å—å†…ã€‚
    """
    warmup_shape = (165000, 2700)
    big = torch.empty(warmup_shape, dtype=torch.bfloat16, device="cuda")

    warmup_start = big.data_ptr()
    warmup_size = big.nbytes
    warmup_end = warmup_start + warmup_size

    print(f"  é¢„çƒ­å—åœ°å€èŒƒå›´: [{warmup_start:#x}, {warmup_end:#x})")
    print(f"  é¢„çƒ­å—å¤§å°: {warmup_size / 1024**2:.1f} MB")

    del big

    print("\nåˆ†é…æµ‹è¯•å¼ é‡å¹¶æ£€æŸ¥åœ°å€")
    all_in_warmup = True
    for key in test_tensors:

        test_tensors[key] = test_tensors[key].to("cuda")
        t = test_tensors[key]

        addr = t.data_ptr()
        size = t.nbytes
        end = addr + size

        in_warmup = (warmup_start <= addr) and (end <= warmup_end)
        status = "âœ… åœ¨é¢„çƒ­å—å†…" if in_warmup else "âŒ åœ¨é¢„çƒ­å—å¤–"

        print(f"  {key}:")
        print(f"    åœ°å€èŒƒå›´: [{addr:#x}, {end:#x}] | {status}")

        if not in_warmup:
            all_in_warmup = False

    if all_in_warmup:
        print("æ‰€æœ‰æµ‹è¯•å¼ é‡çš„ GPU åœ°å€å‡è½åœ¨é¢„çƒ­å¤§å†…å­˜å—å†…ï¼")
    else:
        print("éƒ¨åˆ†å¼ é‡åˆ†é…åœ¨é¢„çƒ­å—ä¹‹å¤–ï¼Œå¯èƒ½å­˜åœ¨ç¢ç‰‡æˆ–å¯¹é½é—®é¢˜ã€‚")


def main():

    print("=" * 20 + "æµ‹è¯•è‡ªå·±å‡½æ•°" + "=" * 20)

    test_tensors1 = create_tensors()
    with search_tensors():
        result_tesnors1 = test_my_method(test_tensors1)
    del test_tensors1, result_tesnors1
    torch.cuda.empty_cache()

    print("=" * 20 + "æµ‹è¯•æ˜¾å­˜é¢„çƒ­" + "=" * 20)

    test_tensors2 = create_tensors()
    with search_tensors():
        result_tesnors2 = test_hf_method(test_tensors2)
    del test_tensors2, result_tesnors2
    torch.cuda.empty_cache()

    print("=" * 20 + "æµ‹è¯•Pytorchåˆ†é…ç­–ç•¥" + "=" * 20)

    test_tensors3 = create_tensors()
    result_tesnors3 = test_torch_allocated(test_tensors3)
    del test_tensors3, result_tesnors3
    torch.cuda.empty_cache()

    print("=" * 20 + "æµ‹è¯•é¢„çƒ­åŠŸèƒ½" + "=" * 20)

    test_tensors4 = create_tensors()
    result_tesnors4 = test_warmup(test_tensors4)
    del test_tensors4, result_tesnors4
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
